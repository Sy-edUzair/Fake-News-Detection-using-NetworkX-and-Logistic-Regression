# -*- coding: utf-8 -*-
"""Graph Theory Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_OCBbIKTuJwjrfnzRGLt6d4kKsMmDiE
"""



pip install pandas networkx scikit-learn matplotlib

import kagglehub

path = kagglehub.dataset_download("bhavikjikadara/fake-news-detection")

import pandas as pd

# Load dataset
df_fake = pd.read_csv(f'{path}/fake.csv')  # Adjust the file name/path if needed
df_true = pd.read_csv(f'{path}/true.csv')  # Adjust the file name/path if needed

df_fake['label'] = 0
df_true['label'] = 1

data = pd.concat([df_fake, df_true], ignore_index=True)
data = data.sample(frac=1).reset_index(drop=True)
print(data.head())

print(data['label'].value_counts())
#sample data

data = data.sample(frac=0.1, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

# Create the graph
news_graph = nx.Graph()

# Add nodes (one for each article)
for index, row in data.iterrows():
    news_graph.add_node(index, label=row['label'])

# Add edges for shared subjects
subject_groups = data.groupby('subject')
for subject, group in subject_groups:
    indices = group.index
    for i in range(len(indices)):
        for j in range(i + 1, len(indices)):
            news_graph.add_edge(indices[i], indices[j])

# Add edges based on title similarity
vectorizer = TfidfVectorizer(stop_words='english')
title_vectors = vectorizer.fit_transform(data['title'])
similarity_matrix = cosine_similarity(title_vectors)

# Lower threshold for similarity
similarity_threshold = 0.1
for i in range(similarity_matrix.shape[0]):
    for j in range(i + 1, similarity_matrix.shape[1]):
        if similarity_matrix[i, j] > similarity_threshold:
            news_graph.add_edge(i, j)

# Print updated graph properties
print(f"Graph has {news_graph.number_of_nodes()} nodes and {news_graph.number_of_edges()} edges.")

import matplotlib.pyplot as plt
pos = nx.spring_layout(news_graph)
nx.draw(news_graph, pos, with_labels=False, node_size=20, edge_color='gray')
plt.show()

# Step 3: Extract network features
degree_centrality = nx.degree_centrality(news_graph)
clustering_coefficient = nx.clustering(news_graph)

  # Drop rows with NaN values

# Prepare the feature matrix
features = pd.DataFrame({
    'degree_centrality': pd.Series(degree_centrality),
    'clustering_coefficient': pd.Series(clustering_coefficient),
})

# Add the label column
features['label'] = data['label'].reindex(features.index)

features = features.dropna()
# Display the feature matrix
print(features.isnull().sum())
print(features.head())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score

# Step 4: Split the data into training and testing sets
X = features[['degree_centrality','clustering_coefficient']]
y = features['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values in y_train (choose ONE of the following strategies)
#Strategy 1: Remove rows with missing values
train_data = pd.concat([X_train, y_train], axis=1).dropna()
X_train = train_data[['degree_centrality','clustering_coefficient']]
y_train = train_data['label']

# Train a logistic regression model
model = LogisticRegression()
model.fit(X_train,y_train)

# Step 5: Evaluate the model
y_pred = model.predict(X_test)

# Evaluate performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

#Visualize Results
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

